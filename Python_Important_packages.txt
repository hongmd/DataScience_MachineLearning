termcolor
loguru
numpy 
"jax[cuda12]" or "jax[rocm]"
pandas datar[pandas] polars tidypolars4sci
matplotlib seaborn plotnine plotly
scipy statsmodels
sympy cvxpy
scikit-learn
imblearn
xgboost
hdbscan
umap


###############################
## install using conda-forge ##
###############################

Use conda-forge for stable version

1) install miniconda or conda

2) Go to (base) environment

3) Run this command to create a conda environment and install libraries: 
   conda create -c conda-forge -n data python=3.12 termcolor loguru numpy jaxlib=*=*cuda* jax cuda-nvcc pandas datar[pandas] polars tidypolars4sci matplotlib seaborn plotnine plotly scipy statsmodels sympy cvxpy scikit-learn imbalanced-learn xgboost hdbscan umap

4) install jax
   pip3 install -U "jax[cuda12]" # for NVIDIA user
   pip3 install -U jax[rocm] # for AMD user


########################
## install using pip3 ##
########################

Use pip3 install -U for latest versions

1) install miniconda or conda

2) Go to (base) environment

3) Create conda environment:
   conda create -n data python=3.12

4) Activate the environment:
   conda activate data

5) Install libraries:
   pip3 install -U termcolor loguru numpy pandas datar[pandas] polars tidypolars4sci matplotlib seaborn plotnine plotly scipy statsmodels sympy cvxpy scikit-learn imblearn xgboost hdbscan

6) install jax
   pip3 install -U "jax[cuda12]" # for NVIDIA user
   pip3 install -U jax[rocm] # for AMD user


#-----------------------------------------------------------------------------------------------------------#
#----------------------------- Jax setup for AMD ROCM (fedora command) -------------------------------------#
#-----------------------------------------------------------------------------------------------------------#

# If the jax.devices() does not detect gpu [RocmDevice(id=0)]
# => do the following steps

# 1. Install all ROCm dependencies at once
sudo dnf install hipsolver hipsolver-devel \
                 hipfft hipfft-devel \
                 miopen miopen-devel \
                 hipblaslt hipblaslt-devel \
                 lld

# 1. Alternative: Install complete ROCm stack
sudo dnf install rocm rocm-devel lld

# 2. While in your conda environment, run these commands to set it permanently
conda env config vars set LLVM_PATH=/usr
conda deactivate
conda activate your_env_name
echo $LLVM_PATH  # Should show /usr

# 3. Run verification command
import jax
import jax.numpy as jnp
from loguru import logger

logger.debug(f'Available devices: {jax.devices()}') # It should be [RocmDevice(id=0)]
logger.debug(f'Default device: {jax.default_backend()}')

# Test GPU computation
x = jnp.arange(10)
y = x ** 2
logger.info(f'GPU computation result: {y}')
logger.debug(f'Computation device: {y.device}')


#---------------------------------------------------------------------------------------------#
#-------------------------- Pytorch for AMD ROCm (fedora command) ----------------------------#
#---------------------------------------------------------------------------------------------#

# Remove malformed AMD repo files causing DNF errors:
sudo rm /etc/yum.repos.d/amdgpu-install.repo
sudo dnf clean all
sudo dnf makecache

# Use Fedoraâ€™s native ROCm packages for compatibility:
sudo dnf install rocm-dkms rocm-dev rocm-utils rocm-opencl rocm-libs hipsparselt

# If hipsparselt is missing, build from source:
git clone https://github.com/ROCm/hipSPARSELt.git
cd hipSPARSELt
./install.sh -idc

# Add ROCm libs to LD_LIBRARY_PATH and set ROCM_PATH:
export ROCM_PATH=/opt/rocm-6.3.4
export PATH=$ROCM_PATH/bin:$PATH
export LD_LIBRARY_PATH=$ROCM_PATH/lib:$ROCM_PATH/hipsparselt/lib:$LD_LIBRARY_PATH
# put in ~/.bashrc to make it permanently

# check
rocminfo
clinfo

# download pytorch
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.3.4/

# Export PATH
nano ~/.bashrc

# Add these to the .bashrc
export HSA_OVERRIDE_GFX_VERSION=11.0.0 # Tells ROCm to treat your gfx1151 GPU as gfx1100 (standard RDNA 3), which has mature kernel support
export AMD_SERIALIZE_KERNEL=3 # Forces kernel serialization for debugging and stability
export HIP_VISIBLE_DEVICES=0 # Explicitly makes your GPU visible to HIP runtime
export ROC_ENABLE_PRE_VEGA=1 #  Enables broader compatibility mode

# Run this to enable above settings
source ~/.bashrc

# test torch GPU detection
python3
import torch
from loguru import logger

logger.debug(torch.cuda.is_available())  # Should print True

# Test if you can actually use the GPU
try:
    x = torch.tensor([1.0, 2.0]).cuda()
    logger.info(f"GPU tensor test successful: {x}")
    logger.debug(f"Current GPU device: {torch.cuda.current_device()}")
except Exception as e:
    logger.error(f"GPU tensor test failed: {e}")