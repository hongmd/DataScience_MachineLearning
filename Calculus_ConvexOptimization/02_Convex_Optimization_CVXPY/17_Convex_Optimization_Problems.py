'''
================================================================================

LIST OF COMMON CONVEX OPTIMIZATION PROBLEM TYPES

================================================================================

- Linear Programming (LP)
- Linear-fractional program
- Robust Linear Programming (RLP)
- Stochastic Robust Linear Programming
- Quadratic Programming (QP)
- Quadratically Constrained Quadratic Programming (QCQP)
- Second-Order Cone Programming (SOCP)
- Deterministic Robust LP via SOCP
- Stochastic Robust LP via SOCP
- Geometric Programming (GP)
- Geometric program in convex form
- Convex optimization with generalized inequality constraints
- Semidefinite Programming (SDP)
- LP and SOCP as SDP
- Vector optimization

================================================================================
'''

#################################################################################

'''
================================================================================
LINEAR PROGRAMMING (LP)
================================================================================
Linear programming involves optimizing a linear objective function 
subject to linear equality and inequality constraints. 

It can be expressed in standard form as:

Problem:
    minimize    c·µÄx + d
    subject to  Gx ‚âº h
                Ax = b

Notes:
    - convex problem with affine objective and constraint functions
    - feasible set is a polyhedron

Brief explanations:
    - Linear objective function ensures constant rate of change (no curvature)
    - All constraints are linear, forming hyperplanes that intersect to create polyhedra
    - Optimal solution always occurs at vertices (extreme points) of the feasible region
    - Simplex method traverses vertices to find optimal solution in finite steps
    - Interior-point methods approach optimum through the interior of feasible region
    - Strong duality holds: primal and dual optimal values are equal (if finite)
    - Can be solved in polynomial time using interior-point algorithms
    - Widely applicable: resource allocation, transportation, production planning
    - Forms the foundation for more complex optimization problems (IP, QP, SDP)
    - Degeneracy occurs when multiple constraints are active at optimal vertex
    - Unbounded problems have no finite optimal value in the objective direction


================================================================================
Linear-fractional program
================================================================================

Problem:
    f‚ÇÄ(x) = (c·µÄx + d)/(e·µÄx + f),    dom f‚ÇÄ(x) = {x | e·µÄx + f > 0}

Properties:
    - a quasiconvex optimization problem; can be solved by bisection
    - also equivalent to the LP (variables y, z)

Equivalent LP formulation:
    minimize    c·µÄy + dz
    subject to  Gy ‚âº hz
                Ay = bz
                e·µÄy + fz = 1
                z ‚â• 0

Brief explanations:
    - The objective function is a ratio of two affine functions, making it quasiconvex
    - Quasiconvexity means sublevel sets are convex, enabling efficient bisection methods
    - The bisection algorithm repeatedly solves convex feasibility problems to find optimal value
    - The equivalent LP uses a change of variables: y = x/(e·µÄx + f), z = 1/(e·µÄx + f)
    - This transformation converts the fractional problem to a standard linear program
    - The constraint e·µÄy + fz = 1 normalizes the denominator to unity
    - Both formulations have the same optimal value and solution structure

=================================================================================
Robust Linear Programming (RLP)
=================================================================================

Description:
Optimization parameters often have uncertainty (e.g., in an LP, c, a·µ¢, b·µ¢ are uncertain). 

Two common models handle uncertainty in a·µ¢:

----------------------------------------------
Deterministic (worst-case) model:
Problem:
    minimize    c·µÄx
    subject to  a·µ¢·µÄx ‚â§ b·µ¢   for all a·µ¢ ‚àà Œï·µ¢, i = 1,‚Ä¶,m

Brief explanations:
    - Ensures constraints hold under every possible realization of a·µ¢ in the uncertainty set Œï·µ¢  
    - Converts to a convex problem if Œï·µ¢ is a convex set (e.g., ellipsoid or polyhedron)  
    - Yields solutions that are immune to worst-case data variations  

----------------------------------------------
Stochastic (chance-constrained) model:
Problem:
    minimize    c·µÄx
    subject to  Prob(a·µ¢·µÄx ‚â§ b·µ¢) ‚â• Œ∑, i = 1,‚Ä¶,m

Brief explanations:
    - a·µ¢ is treated as a random variable with known distribution  
    - Constraints must be satisfied with confidence level Œ∑ (e.g., 95%)  
    - Often approximated or relaxed to tractable convex constraints (e.g., using Chebyshev or scenario approaches)  
    - Balances risk and performance by allowing small probability of constraint violation  

'''

#################################################################################

'''
================================================================================
QUADRATIC PROGRAMMING (QP)
================================================================================

Problem:
    minimize    (1/2)x·µÄPx + q·µÄx + r
    subject to  Gx ‚âº h
                Ax = b

Properties:
    - P ‚àà S‚Çä‚Åø, so objective is convex quadratic
    - minimize a convex quadratic function over a polyhedron

Brief explanations:
    - The matrix P must be positive semidefinite (P ‚™∞ 0) for convexity
    - Positive semidefinite means x·µÄPx ‚â• 0 for all vectors x
    - This ensures the objective function has a single global minimum (no local minima)
    - The feasible region is still a polyhedron defined by linear constraints
    - Can be solved efficiently using interior-point methods or active-set methods
    - Interior-point methods have polynomial-time complexity: O(n¬≥L) operations
    - When P = 0, the problem reduces to a linear program (LP)
    - The quadratic term (1/2)x·µÄPx represents curvature in the objective function
    - Applications include portfolio optimization, machine learning, and control theory
    - KKT conditions provide necessary and sufficient optimality conditions for convex QPs

================================================================================
QUADRATICALLY CONSTRAINED QUADRATIC PROGRAMMING (QCQP)
================================================================================

Problem:
    minimize    (1/2)x·µÄP‚ÇÄx + q‚ÇÄ·µÄx + r‚ÇÄ
    subject to  (1/2)x·µÄP·µ¢x + q·µ¢·µÄx + r·µ¢ ‚â§ 0,    i = 1,...,m
                Ax = b

Properties:
    - P·µ¢ ‚àà S‚Çä‚Åø: objective and constraints are convex quadratic
    - if P‚ÇÅ,...,P‚Çò ‚àà S‚Çä‚Åø, feasible region is intersection of m ellipsoids and 
      an affine set

Brief explanations:
    - Extension of QP where constraints are also quadratic instead of just linear
    - Convexity requires ALL matrices P‚ÇÄ, P‚ÇÅ,..., P‚Çò to be positive semidefinite
    - When convex, feasible region is intersection of ellipsoidal sets and hyperplanes
    - Each quadratic constraint (1/2)x·µÄP·µ¢x + q·µ¢·µÄx + r·µ¢ ‚â§ 0 defines an ellipsoid
    - Convex QCQP can be solved efficiently using interior-point methods
    - Non-convex QCQP is NP-hard in general (includes binary quadratic programming)
    - Common relaxation technique: Shor's semidefinite programming (SDP) relaxation
    - SDP relaxation provides lower bounds by lifting to matrix variable X = xx·µÄ
    - Applications: portfolio optimization, robust optimization, signal processing
    - Can model problems with uncertainty and risk through quadratic constraints
    - Special cases include trust region subproblems and least squares with constraints
'''

#################################################################################

'''
================================================================================
SECOND-ORDER CONE PROGRAMMING (SOCP)
================================================================================

Problem:
    minimize    f·µÄx
    subject to  ‚ÄñA·µ¢x + b·µ¢‚Äñ‚ÇÇ ‚â§ c·µ¢·µÄx + d·µ¢,    i = 1,...,m
                Fx = g

(A·µ¢ ‚àà ‚Ñù‚Åø‚Å±À£‚Åø, F ‚àà ‚Ñù·µñÀ£‚Åø)

Properties:
    - inequalities are called second-order cone (SOC) constraints:
      (A·µ¢x + b·µ¢, c·µ¢·µÄx + d·µ¢) ‚àà second-order cone in ‚Ñù‚Åø‚Å±‚Å∫¬π
    - for n·µ¢ = 0, reduces to an LP; if c·µ¢ = 0, reduces to a QCQP
    - more general than QCQP and LP

Brief explanations:
    - Each SOC constraint defines a convex cone: {(u,t) | ‚Äñu‚Äñ‚ÇÇ ‚â§ t} in ‚Ñù‚Åø‚Å±‚Å∫¬π
    - The feasible region is intersection of multiple cones with an affine subspace
    - SOC constraints generalize linear constraints (when n·µ¢ = 0) and quadratic constraints
    - Convex optimization problem solvable by interior-point methods with polynomial complexity
    - Efficient primal-dual algorithms achieve O(‚àön log(1/Œµ)) iteration complexity
    - Can represent many practical constraints: robust optimization, norm minimization
    - Special case of semidefinite programming (SDP) but computationally more efficient
    - Applications include portfolio optimization, robust linear programming, filter design
    - When c·µ¢ = 0, constraint becomes ‚ÄñA·µ¢x + b·µ¢‚Äñ‚ÇÇ ‚â§ d·µ¢ (simple norm bound)
    - Bridges the gap between linear programming (simple) and semidefinite programming (general)
    - Can model sum-of-norms objectives and max-of-norms constraints efficiently

    
================================================================================
Deterministic robust LP via SOCP
================================================================================

Uncertainty set (ellipsoidal):
    Œï·µ¢ = {ƒÅ·µ¢ + P·µ¢u  |  ‚Äñu‚Äñ‚ÇÇ ‚â§ 1}    (ƒÅ·µ¢ ‚àà ‚Ñù‚Åø, P·µ¢ ‚àà ‚Ñù‚ÅøÀ£‚Åø)

Original robust LP:
    minimize    c·µÄx
    subject to  a·µ¢·µÄx ‚â§ b·µ¢   ‚àÄ a·µ¢ ‚àà Œï·µ¢,    i = 1,‚Ä¶,m

Equivalent SOCP:
    minimize    c·µÄx
    subject to  ƒÅ·µ¢·µÄx + ‚ÄñP·µ¢·µÄx‚Äñ‚ÇÇ ‚â§ b·µ¢,    i = 1,‚Ä¶,m

Brief explanations:
    - Ellipsoid Œï·µ¢ captures uncertainty around nominal ƒÅ·µ¢ with shape P·µ¢  
    - Worst-case constraint sup‚Çç‚Äñu‚Äñ‚ÇÇ‚â§1‚Çé(ƒÅ·µ¢ + P·µ¢u)·µÄx = ƒÅ·µ¢·µÄx + ‚ÄñP·µ¢·µÄx‚Äñ‚ÇÇ  
    - Reformulates infinite constraints into m convex SOC constraints  
    - SOCP can be solved efficiently with interior-point methods  
    - Robust solution x is safeguarded against all a·µ¢ within ellipsoids  
    - Trade-off between robustness and conservatism controlled by P·µ¢  

================================================================================
Stochastic robust LP via SOCP
================================================================================

Assumptions:
    a·µ¢ ~ ùí©(ƒÅ·µ¢, Œ£·µ¢)  
    ‚áí a·µ¢·µÄx ~ ùí©(ƒÅ·µ¢·µÄx, x·µÄŒ£·µ¢x)  

Chance-constrained robust LP:
    minimize    c·µÄx  
    subject to  Prob(a·µ¢·µÄx ‚â§ b·µ¢) ‚â• Œ∑,    i = 1,‚Ä¶,m  

Probability expression:
    Prob(a·µ¢·µÄx ‚â§ b·µ¢) = Œ¶( (b·µ¢ - ƒÅ·µ¢·µÄx) / ‚ÄñŒ£·µ¢¬π·êü¬≤ x‚Äñ‚ÇÇ )  
    where Œ¶(t) = (1/‚àö(2œÄ)) ‚à´‚Çã‚àû·µó e^(-u¬≤/2) du  

Equivalent SOCP (Œ∑ ‚â• ¬Ω):
    minimize    c·µÄx  
    subject to  ƒÅ·µ¢·µÄx + Œ¶‚Åª¬π(Œ∑) ‚ÄñŒ£·µ¢¬π·êü¬≤ x‚Äñ‚ÇÇ ‚â§ b·µ¢,    i = 1,‚Ä¶,m  

Brief explanations:
    - Œ¶‚Åª¬π(Œ∑) is the Gaussian quantile for confidence level Œ∑  
    - Reformulates chance constraint into a single SOC constraint per i  
    - Ensures constraint holds with at least probability Œ∑  
    - Higher Œ∑ ‚áí more conservative (larger Œ¶‚Åª¬π(Œ∑))  
    - Solvable efficiently by interior-point SOCP solvers  
    - Balances risk (violation probability) against optimality  
'''

##################################################################################

'''
================================================================================
Geometric Programming (GP)
================================================================================

Definitions:
    Monomial function:
        f(x) = c¬∑x‚ÇÅ·µÉ¬π x‚ÇÇ·µÉ¬≤ ‚Ä¶ x‚Çô·µÉ‚Åø,    dom f = ‚Ñù‚Çä‚Çä‚Åø
        with c > 0; exponents a·µ¢ can be any real number
    Posynomial function:
        f(x) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ c‚Çñ¬∑x‚ÇÅ·µÉ‚ÇÅ‚Çñ x‚ÇÇ·µÉ‚ÇÇ‚Çñ ‚Ä¶ x‚Çô·µÉ‚Çô‚Çñ,    dom f = ‚Ñù‚Çä‚Çä‚Åø
        sum of monomials

Problem (GP):
    minimize    f‚ÇÄ(x)
    subject to  f·µ¢(x) ‚â§ 1,    i = 1,‚Ä¶,m
                h·µ¢(x) = 1,    i = 1,‚Ä¶,p
    where f·µ¢ are posynomial, h·µ¢ are monomial

Brief explanations:
    - Variables and functions must be positive (x ‚àà ‚Ñù‚Çä‚Çä‚Åø)  
    - Posynomials are log-log convex: under change of variables u = log x, objective and ‚â§ constraints become convex  
    - Equality constraints h·µ¢(x)=1 (monomials) become affine in u-space  
    - GP can be transformed into a convex optimization problem via logarithmic change of variables  
    - Solvable efficiently by interior-point methods after transformation  
    - Applications: circuit design, resource allocation, chemical engineering  
    - Allows modeling of power-law relationships and multiplicative trade-offs  

================================================================================
Geometric program in convex form
================================================================================

Change of variables and log transformation:
    y·µ¢ = log x·µ¢,    b‚Çñ = log c‚Çñ

Monomial:
    f(x) = c¬∑x‚ÇÅ·µÉ¬π ‚Ä¶ x‚Çô·µÉ‚Åø  
    ‚áí log f(e ∏) = a·µÄy + b

Posynomial:
    f(x) = ‚àë‚Çñ‚Çå‚ÇÅ·¥∑ c‚Çñ¬∑x‚ÇÅ·µÉ‚ÇÅ‚Çñ ‚Ä¶ x‚Çô·µÉ‚Çô‚Çñ  
    ‚áí log f(e ∏) = log(‚àë‚Çñ‚Çå‚ÇÅ·¥∑ exp(a‚Çñ·µÄy + b‚Çñ))

Convex form GP:
    minimize    log(‚àë‚Çñ‚Çå‚ÇÅ·¥∑ exp(a‚ÇÄ‚Çñ·µÄy + b‚ÇÄ‚Çñ))  
    subject to  log(‚àë‚Çñ‚Çå‚ÇÅ·¥∑ exp(a·µ¢‚Çñ·µÄy + b·µ¢‚Çñ)) ‚â§ 0,    i = 1,‚Ä¶,m  
                Gy + d = 0

Brief explanations:
    - Log-sum-exp is convex and represents posynomial ‚â§1 constraints  
    - Equality constraints Gy + d = 0 are affine in y  
    - Entire problem is a convex optimization in y-space  
    - Solvable by standard interior-point or first-order methods  
    - Transformation leverages log-log convexity of posynomials  
    - Guarantees globally optimal solution for original GP  
'''

##################################################################################

'''
================================================================================
Convex optimization with generalized inequality constraints
================================================================================

Problem (general form):
    minimize    f‚ÇÄ(x)
    subject to  f·µ¢(x) ‚âº_{K·µ¢} 0,    i = 1,‚Ä¶,m
                Ax = b

    ‚Ä¢ f‚ÇÄ: ‚Ñù‚Åø ‚Üí ‚Ñù is convex  
    ‚Ä¢ f·µ¢: ‚Ñù‚Åø ‚Üí ‚Ñù·µè‚Å± is K·µ¢-convex w.r.t. proper cone K·µ¢  
    ‚Ä¢ ‚Äú‚âº_{K·µ¢} 0‚Äù means f·µ¢(x) ‚àà -K·µ¢  

Conic form special case:
    minimize    c·µÄx
    subject to  Fx + g ‚âº_{K} 0
                Ax = b

    ‚Ä¢ Extends LP (K = ‚Ñù‚Çä·µê) to nonpolyhedral cones (SOC, SDP, etc.)

Brief explanations:
    - Generalized inequalities replace scalar ‚â§ with vector inequalities over cones  
    - Proper cone K·µ¢ defines a partial order: u ‚âº_{K·µ¢} v ‚áî v-u ‚àà K·µ¢  
    - Conic form has affine objectives/constraints, unifies LP, SOCP, SDP  
    - Convex feasible set; any local minimum is global  
    - Duality theory extends: conic duals give strong duality under Slater's condition  
    - Cone examples: nonnegative orthant (LP), second-order cone (SOCP), positive semidefinite cone (SDP)  
    - Enables modeling of a wide range of convex constraints in a unified framework  
'''

##################################################################################
'''
================================================================================
Semidefinite Programming (SDP)
================================================================================

Problem:
    minimize    c·µÄx  
    subject to  x‚ÇÅF‚ÇÅ + x‚ÇÇF‚ÇÇ + ‚Ä¶ + x‚ÇôF‚Çô + G ‚âº‚Çõ‚Çä 0  
                Ax = b  
    (F·µ¢, G ‚àà S·µè)

Notes:
    - Inequality constraint is a linear matrix inequality (LMI): X ‚âº‚Çõ‚Çä 0 ‚áî X is positive semidefinite  
    - Can include multiple LMIs by block-diagonal aggregation into a single LMI  

Brief explanations:
    - Extends conic form to the positive semidefinite cone S‚Çä·µè  
    - LMI constraints model requirements on eigenvalues of affine matrix expressions  
    - Convex problem: feasible set is an intersection of affine ‚Äúslices‚Äù of the PSD cone  
    - Solvable by interior-point methods with polynomial-time complexity  
    - Duality yields semidefinite dual problem with strong duality under Slater‚Äôs condition  
    - Applications: control system design (Lyapunov inequalities), covariance estimation, quantum information  
    - Special cases: when k=1 reduces to an SOCP (scalar PSD cone), when F·µ¢ are diagonal reduces to an LP  
    - SDP relaxations provide tractable bounds for hard combinatorial problems (e.g., Max-Cut)  
'''

##################################################################################

'''
================================================================================
LP and SOCP as SDP
================================================================================

LP and equivalent SDP:
    LP:
        minimize    c·µÄx  
        subject to  Ax ‚âº b  
    SDP:
        minimize    c·µÄx  
        subject to  diag(Ax - b) ‚âº‚Çõ‚Çä 0  

SOCP and equivalent SDP:
    SOCP:
        minimize    f·µÄx  
        subject to  ‚ÄñA·µ¢x + b·µ¢‚Äñ‚ÇÇ ‚â§ c·µ¢·µÄx + d·µ¢,    i = 1,‚Ä¶,m  
    SDP:
        minimize    f·µÄx  
        subject to  [ (c·µ¢·µÄx + d·µ¢) I      A·µ¢x + b·µ¢  
                      (A·µ¢x + b·µ¢)·µÄ   (c·µ¢·µÄx + d·µ¢) ] ‚âΩ‚Çõ‚Çä 0,    i = 1,‚Ä¶,m  

Brief explanations:
    - LP can be viewed as an SDP over the diagonal PSD cone (scalar constraints become one-by-one LMIs)  
    - diag(Ax-b)‚âº‚Çõ‚Çä0 enforces each component (Ax-b)·µ¢ ‚â§ 0 via 1x1 PSD blocks  
    - SOCP constraints ‚Äñu‚Äñ‚ÇÇ ‚â§ t are equivalent to 2x2 PSD constraints on [ tI u; u·µÄ t ]  
    - Embedding LP and SOCP in SDP unifies all as conic problems over the PSD cone  
    - Enables use of general-purpose SDP solvers for a wider class of problems  
'''

##################################################################################

'''
================================================================================
Vector optimization
================================================================================

General vector optimization problem:
    minimize (w.r.t. K)    f‚ÇÄ(x)
    subject to             f·µ¢(x) ‚âº 0,    i = 1,‚Ä¶,m  
                            h·µ¢(x) = 0, i = 1,‚Ä¶,p  

    ‚Ä¢ f‚ÇÄ: ‚Ñù‚Åø ‚Üí ‚Ñù·µ† is vector‚Äêvalued  
    ‚Ä¢ ‚Äúminimize w.r.t. K‚Äù means find x such that f‚ÇÄ(x) is minimal under the partial order defined by proper cone K ‚äÜ ‚Ñù·µ†  
    ‚Ä¢ f·µ¢: ‚Ñù‚Åø ‚Üí ‚Ñù are scalar convex functions  

Convex vector optimization problem:
    minimize (w.r.t. K)    f‚ÇÄ(x)
    subject to             f·µ¢(x) ‚â§ 0,   i = 1,‚Ä¶,m  
                            A x = b  

    ‚Ä¢ f‚ÇÄ is K‚Äêconvex: for all x,y and Œ∏‚àà[0,1],  
        f‚ÇÄ(Œ∏x+(1‚ÄìŒ∏)y) ‚âº Œ∏‚Äâf‚ÇÄ(x)+(1‚ÄìŒ∏)‚Äâf‚ÇÄ(y)  
    ‚Ä¢ scalar constraints f·µ¢ convex, equality constraints affine  

Brief explanations:
    - Objectives are vector‚Äêvalued; optimality means no other feasible point yields a strictly smaller vector in cone ordering  
    - Trade‚Äêoffs between objectives characterized by Pareto frontier (set of nondominated solutions)  
    - Proper cone K (e.g., nonnegative orthant) defines preference direction in objective space  
    - Can be scalarized via weighted sums or Œµ‚Äêconstraint methods to compute Pareto‚Äêoptimal points  
    - Convexity ensures convex Pareto frontier and tractable computation of supported efficient points  
    - Applications: multi‚Äêcriteria decision making, game theory, economics, engineering design  
'''