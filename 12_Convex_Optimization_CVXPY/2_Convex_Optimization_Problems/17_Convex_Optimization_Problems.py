'''
================================================================================

LIST OF COMMON CONVEX OPTIMIZATION PROBLEM TYPES

================================================================================

- Linear Programming (LP)
    Linear-fractional program
    Robust Linear Programming (RLP)
    Stochastic Robust Linear Programming

- Quadratic Programming (QP)
    Quadratically Constrained Quadratic Programming (QCQP)

- Second-Order Cone Programming (SOCP)
    Deterministic Robust LP via SOCP
    Stochastic Robust LP via SOCP

- Geometric Programming (GP)
    Geometric program in convex form
    Convex optimization with generalized inequality constraints

- Semidefinite Programming (SDP)

- LP and SOCP as SDP

- Vector optimization
    Optimal and Pareto optimal points
    Multicriterion (multi-objective) optimization

================================================================================
'''

#################################################################################

'''
================================================================================
LINEAR PROGRAMMING (LP)
================================================================================
Linear programming involves optimizing a linear objective function 
subject to linear equality and inequality constraints. 

It can be expressed in standard form as:

Problem:
    minimize    cáµ€x + d
    subject to  Gx â‰¼ h
                Ax = b

Notes:
    - convex problem with affine objective and constraint functions
    - feasible set is a polyhedron

Brief explanations:
    - Linear objective function ensures constant rate of change (no curvature)
    - All constraints are linear, forming hyperplanes that intersect to create polyhedra
    - Optimal solution always occurs at vertices (extreme points) of the feasible region
    - Simplex method traverses vertices to find optimal solution in finite steps
    - Interior-point methods approach optimum through the interior of feasible region
    - Strong duality holds: primal and dual optimal values are equal (if finite)
    - Can be solved in polynomial time using interior-point algorithms
    - Widely applicable: resource allocation, transportation, production planning
    - Forms the foundation for more complex optimization problems (IP, QP, SDP)
    - Degeneracy occurs when multiple constraints are active at optimal vertex
    - Unbounded problems have no finite optimal value in the objective direction


================================================================================
Linear-fractional program
================================================================================

Problem:
    fâ‚€(x) = (cáµ€x + d)/(eáµ€x + f),    dom fâ‚€(x) = {x | eáµ€x + f > 0}

Properties:
    - a quasiconvex optimization problem; can be solved by bisection
    - also equivalent to the LP (variables y, z)

Equivalent LP formulation:
    minimize    cáµ€y + dz
    subject to  Gy â‰¼ hz
                Ay = bz
                eáµ€y + fz = 1
                z â‰¥ 0

Brief explanations:
    - The objective function is a ratio of two affine functions, making it quasiconvex
    - Quasiconvexity means sublevel sets are convex, enabling efficient bisection methods
    - The bisection algorithm repeatedly solves convex feasibility problems to find optimal value
    - The equivalent LP uses a change of variables: y = x/(eáµ€x + f), z = 1/(eáµ€x + f)
    - This transformation converts the fractional problem to a standard linear program
    - The constraint eáµ€y + fz = 1 normalizes the denominator to unity
    - Both formulations have the same optimal value and solution structure

=================================================================================
Robust Linear Programming (RLP)
=================================================================================

Description:
Optimization parameters often have uncertainty (e.g., in an LP, c, aáµ¢, báµ¢ are uncertain). 

Two common models handle uncertainty in aáµ¢:

----------------------------------------------
Deterministic (worst-case) model:
Problem:
    minimize    cáµ€x
    subject to  aáµ¢áµ€x â‰¤ báµ¢   for all aáµ¢ âˆˆ Î•áµ¢, i = 1,â€¦,m

Brief explanations:
    - Ensures constraints hold under every possible realization of aáµ¢ in the uncertainty set Î•áµ¢  
    - Converts to a convex problem if Î•áµ¢ is a convex set (e.g., ellipsoid or polyhedron)  
    - Yields solutions that are immune to worst-case data variations  

----------------------------------------------
Stochastic (chance-constrained) model:
Problem:
    minimize    cáµ€x
    subject to  Prob(aáµ¢áµ€x â‰¤ báµ¢) â‰¥ Î·, i = 1,â€¦,m

Brief explanations:
    - aáµ¢ is treated as a random variable with known distribution  
    - Constraints must be satisfied with confidence level Î· (e.g., 95%)  
    - Often approximated or relaxed to tractable convex constraints (e.g., using Chebyshev or scenario approaches)  
    - Balances risk and performance by allowing small probability of constraint violation  

'''

#################################################################################

'''
================================================================================
QUADRATIC PROGRAMMING (QP)
================================================================================

Problem:
    minimize    (1/2)xáµ€Px + qáµ€x + r
    subject to  Gx â‰¼ h
                Ax = b

Properties:
    - P âˆˆ Sâ‚Šâ¿, so objective is convex quadratic
    - minimize a convex quadratic function over a polyhedron

Brief explanations:
    - The matrix P must be positive semidefinite (P âª° 0) for convexity
    - Positive semidefinite means xáµ€Px â‰¥ 0 for all vectors x
    - This ensures the objective function has a single global minimum (no local minima)
    - The feasible region is still a polyhedron defined by linear constraints
    - Can be solved efficiently using interior-point methods or active-set methods
    - Interior-point methods have polynomial-time complexity: O(nÂ³L) operations
    - When P = 0, the problem reduces to a linear program (LP)
    - The quadratic term (1/2)xáµ€Px represents curvature in the objective function
    - Applications include portfolio optimization, machine learning, and control theory
    - KKT conditions provide necessary and sufficient optimality conditions for convex QPs

================================================================================
QUADRATICALLY CONSTRAINED QUADRATIC PROGRAMMING (QCQP)
================================================================================

Problem:
    minimize    (1/2)xáµ€Pâ‚€x + qâ‚€áµ€x + râ‚€
    subject to  (1/2)xáµ€Páµ¢x + qáµ¢áµ€x + ráµ¢ â‰¤ 0,    i = 1,...,m
                Ax = b

Properties:
    - Páµ¢ âˆˆ Sâ‚Šâ¿: objective and constraints are convex quadratic
    - if Pâ‚,...,Pâ‚˜ âˆˆ Sâ‚Šâ¿, feasible region is intersection of m ellipsoids and 
      an affine set

Brief explanations:
    - Extension of QP where constraints are also quadratic instead of just linear
    - Convexity requires ALL matrices Pâ‚€, Pâ‚,..., Pâ‚˜ to be positive semidefinite
    - When convex, feasible region is intersection of ellipsoidal sets and hyperplanes
    - Each quadratic constraint (1/2)xáµ€Páµ¢x + qáµ¢áµ€x + ráµ¢ â‰¤ 0 defines an ellipsoid
    - Convex QCQP can be solved efficiently using interior-point methods
    - Non-convex QCQP is NP-hard in general (includes binary quadratic programming)
    - Common relaxation technique: Shor's semidefinite programming (SDP) relaxation
    - SDP relaxation provides lower bounds by lifting to matrix variable X = xxáµ€
    - Applications: portfolio optimization, robust optimization, signal processing
    - Can model problems with uncertainty and risk through quadratic constraints
    - Special cases include trust region subproblems and least squares with constraints
'''

#################################################################################

'''
================================================================================
SECOND-ORDER CONE PROGRAMMING (SOCP)
================================================================================

Problem:
    minimize    fáµ€x
    subject to  â€–Aáµ¢x + báµ¢â€–â‚‚ â‰¤ cáµ¢áµ€x + dáµ¢,    i = 1,...,m
                Fx = g

(Aáµ¢ âˆˆ â„â¿â±Ë£â¿, F âˆˆ â„áµ–Ë£â¿)

Properties:
    - inequalities are called second-order cone (SOC) constraints:
      (Aáµ¢x + báµ¢, cáµ¢áµ€x + dáµ¢) âˆˆ second-order cone in â„â¿â±âºÂ¹
    - for náµ¢ = 0, reduces to an LP; if cáµ¢ = 0, reduces to a QCQP
    - more general than QCQP and LP

Brief explanations:
    - Each SOC constraint defines a convex cone: {(u,t) | â€–uâ€–â‚‚ â‰¤ t} in â„â¿â±âºÂ¹
    - The feasible region is intersection of multiple cones with an affine subspace
    - SOC constraints generalize linear constraints (when náµ¢ = 0) and quadratic constraints
    - Convex optimization problem solvable by interior-point methods with polynomial complexity
    - Efficient primal-dual algorithms achieve O(âˆšn log(1/Îµ)) iteration complexity
    - Can represent many practical constraints: robust optimization, norm minimization
    - Special case of semidefinite programming (SDP) but computationally more efficient
    - Applications include portfolio optimization, robust linear programming, filter design
    - When cáµ¢ = 0, constraint becomes â€–Aáµ¢x + báµ¢â€–â‚‚ â‰¤ dáµ¢ (simple norm bound)
    - Bridges the gap between linear programming (simple) and semidefinite programming (general)
    - Can model sum-of-norms objectives and max-of-norms constraints efficiently

    
================================================================================
Deterministic robust LP via SOCP
================================================================================

Uncertainty set (ellipsoidal):
    Î•áµ¢ = {Äáµ¢ + Páµ¢u  |  â€–uâ€–â‚‚ â‰¤ 1}    (Äáµ¢ âˆˆ â„â¿, Páµ¢ âˆˆ â„â¿Ë£â¿)

Original robust LP:
    minimize    cáµ€x
    subject to  aáµ¢áµ€x â‰¤ báµ¢   âˆ€ aáµ¢ âˆˆ Î•áµ¢,    i = 1,â€¦,m

Equivalent SOCP:
    minimize    cáµ€x
    subject to  Äáµ¢áµ€x + â€–Páµ¢áµ€xâ€–â‚‚ â‰¤ báµ¢,    i = 1,â€¦,m

Brief explanations:
    - Ellipsoid Î•áµ¢ captures uncertainty around nominal Äáµ¢ with shape Páµ¢  
    - Worst-case constraint supâ‚â€–uâ€–â‚‚â‰¤1â‚Ž(Äáµ¢ + Páµ¢u)áµ€x = Äáµ¢áµ€x + â€–Páµ¢áµ€xâ€–â‚‚  
    - Reformulates infinite constraints into m convex SOC constraints  
    - SOCP can be solved efficiently with interior-point methods  
    - Robust solution x is safeguarded against all aáµ¢ within ellipsoids  
    - Trade-off between robustness and conservatism controlled by Páµ¢  

================================================================================
Stochastic robust LP via SOCP
================================================================================

Assumptions:
    aáµ¢ ~ ð’©(Äáµ¢, Î£áµ¢)  
    â‡’ aáµ¢áµ€x ~ ð’©(Äáµ¢áµ€x, xáµ€Î£áµ¢x)  

Chance-constrained robust LP:
    minimize    cáµ€x  
    subject to  Prob(aáµ¢áµ€x â‰¤ báµ¢) â‰¥ Î·,    i = 1,â€¦,m  

Probability expression:
    Prob(aáµ¢áµ€x â‰¤ báµ¢) = Î¦( (báµ¢ - Äáµ¢áµ€x) / â€–Î£áµ¢Â¹áŸÂ² xâ€–â‚‚ )  
    where Î¦(t) = (1/âˆš(2Ï€)) âˆ«â‚‹âˆžáµ— e^(-uÂ²/2) du  

Equivalent SOCP (Î· â‰¥ Â½):
    minimize    cáµ€x  
    subject to  Äáµ¢áµ€x + Î¦â»Â¹(Î·) â€–Î£áµ¢Â¹áŸÂ² xâ€–â‚‚ â‰¤ báµ¢,    i = 1,â€¦,m  

Brief explanations:
    - Î¦â»Â¹(Î·) is the Gaussian quantile for confidence level Î·  
    - Reformulates chance constraint into a single SOC constraint per i  
    - Ensures constraint holds with at least probability Î·  
    - Higher Î· â‡’ more conservative (larger Î¦â»Â¹(Î·))  
    - Solvable efficiently by interior-point SOCP solvers  
    - Balances risk (violation probability) against optimality  
'''

##################################################################################

'''
================================================================================
Geometric Programming (GP)
================================================================================

Definitions:
    Monomial function:
        f(x) = cÂ·xâ‚áµƒÂ¹ xâ‚‚áµƒÂ² â€¦ xâ‚™áµƒâ¿,    dom f = â„â‚Šâ‚Šâ¿
        with c > 0; exponents aáµ¢ can be any real number
    Posynomial function:
        f(x) = âˆ‘â‚–â‚Œâ‚á´· câ‚–Â·xâ‚áµƒâ‚â‚– xâ‚‚áµƒâ‚‚â‚– â€¦ xâ‚™áµƒâ‚™â‚–,    dom f = â„â‚Šâ‚Šâ¿
        sum of monomials

Problem (GP):
    minimize    fâ‚€(x)
    subject to  fáµ¢(x) â‰¤ 1,    i = 1,â€¦,m
                háµ¢(x) = 1,    i = 1,â€¦,p
    where fáµ¢ are posynomial, háµ¢ are monomial

Brief explanations:
    - Variables and functions must be positive (x âˆˆ â„â‚Šâ‚Šâ¿)  
    - Posynomials are log-log convex: under change of variables u = log x, objective and â‰¤ constraints become convex  
    - Equality constraints háµ¢(x)=1 (monomials) become affine in u-space  
    - GP can be transformed into a convex optimization problem via logarithmic change of variables  
    - Solvable efficiently by interior-point methods after transformation  
    - Applications: circuit design, resource allocation, chemical engineering  
    - Allows modeling of power-law relationships and multiplicative trade-offs  

================================================================================
Geometric program in convex form
================================================================================

Change of variables and log transformation:
    yáµ¢ = log xáµ¢,    bâ‚– = log câ‚–

Monomial:
    f(x) = cÂ·xâ‚áµƒÂ¹ â€¦ xâ‚™áµƒâ¿  
    â‡’ log f(eÊ¸) = aáµ€y + b

Posynomial:
    f(x) = âˆ‘â‚–â‚Œâ‚á´· câ‚–Â·xâ‚áµƒâ‚â‚– â€¦ xâ‚™áµƒâ‚™â‚–  
    â‡’ log f(eÊ¸) = log(âˆ‘â‚–â‚Œâ‚á´· exp(aâ‚–áµ€y + bâ‚–))

Convex form GP:
    minimize    log(âˆ‘â‚–â‚Œâ‚á´· exp(aâ‚€â‚–áµ€y + bâ‚€â‚–))  
    subject to  log(âˆ‘â‚–â‚Œâ‚á´· exp(aáµ¢â‚–áµ€y + báµ¢â‚–)) â‰¤ 0,    i = 1,â€¦,m  
                Gy + d = 0

Brief explanations:
    - Log-sum-exp is convex and represents posynomial â‰¤1 constraints  
    - Equality constraints Gy + d = 0 are affine in y  
    - Entire problem is a convex optimization in y-space  
    - Solvable by standard interior-point or first-order methods  
    - Transformation leverages log-log convexity of posynomials  
    - Guarantees globally optimal solution for original GP  
'''

##################################################################################

'''
================================================================================
Convex optimization with generalized inequality constraints
================================================================================

Problem (general form):
    minimize    fâ‚€(x)
    subject to  fáµ¢(x) â‰¼_{Káµ¢} 0,    i = 1,â€¦,m
                Ax = b

    â€¢ fâ‚€: â„â¿ â†’ â„ is convex  
    â€¢ fáµ¢: â„â¿ â†’ â„áµâ± is Káµ¢-convex w.r.t. proper cone Káµ¢  
    â€¢ â€œâ‰¼_{Káµ¢} 0â€ means fáµ¢(x) âˆˆ -Káµ¢  

Conic form special case:
    minimize    cáµ€x
    subject to  Fx + g â‰¼_{K} 0
                Ax = b

    â€¢ Extends LP (K = â„â‚Šáµ) to nonpolyhedral cones (SOC, SDP, etc.)

Brief explanations:
    - Generalized inequalities replace scalar â‰¤ with vector inequalities over cones  
    - Proper cone Káµ¢ defines a partial order: u â‰¼_{Káµ¢} v â‡” v-u âˆˆ Káµ¢  
    - Conic form has affine objectives/constraints, unifies LP, SOCP, SDP  
    - Convex feasible set; any local minimum is global  
    - Duality theory extends: conic duals give strong duality under Slater's condition  
    - Cone examples: nonnegative orthant (LP), second-order cone (SOCP), positive semidefinite cone (SDP)  
    - Enables modeling of a wide range of convex constraints in a unified framework  
'''

##################################################################################
'''
================================================================================
Semidefinite Programming (SDP)
================================================================================

Problem:
    minimize    tr(CX)
    subject to  tr(Aáµ¢ X) = báµ¢,   i = 1, â€¦, p
                X âª° 0,

Where tr() is the trace function, X âˆˆ Sâ¿ is the optimization variable and C, Aâ‚, â€¦, A_p âˆˆ Sâ¿, 
and bâ‚, â€¦, b_p âˆˆ â„ are problem data, and X âª° 0 is a matrix inequality. 
Here Sâ¿ denotes the set of n-by-n symmetric matrices.

Brief explanations:
    - Extends conic form to the positive semidefinite cone Sâ‚Šáµ  
    - LMI constraints model requirements on eigenvalues of affine matrix expressions  
    - Convex problem: feasible set is an intersection of affine â€œslicesâ€ of the PSD cone  
    - Solvable by interior-point methods with polynomial-time complexity  
    - Duality yields semidefinite dual problem with strong duality under Slater's condition  
    - Applications: control system design (Lyapunov inequalities), covariance estimation, quantum information  
    - Special cases: when k=1 reduces to an SOCP (scalar PSD cone), when Fáµ¢ are diagonal reduces to an LP  
    - SDP relaxations provide tractable bounds for hard combinatorial problems (e.g., Max-Cut)  
'''

##################################################################################

'''
================================================================================
LP and SOCP as SDP
================================================================================

LP and equivalent SDP:
    LP:
        minimize    cáµ€x  
        subject to  Ax â‰¼ b  
    SDP:
        minimize    cáµ€x  
        subject to  diag(Ax - b) â‰¼â‚›â‚Š 0  

SOCP and equivalent SDP:
    SOCP:
        minimize    fáµ€x  
        subject to  â€–Aáµ¢x + báµ¢â€–â‚‚ â‰¤ cáµ¢áµ€x + dáµ¢,    i = 1,â€¦,m  
    SDP:
        minimize    fáµ€x  
        subject to  [ (cáµ¢áµ€x + dáµ¢) I      Aáµ¢x + báµ¢  
                      (Aáµ¢x + báµ¢)áµ€   (cáµ¢áµ€x + dáµ¢) ] â‰½â‚›â‚Š 0,    i = 1,â€¦,m  

Brief explanations:
    - LP can be viewed as an SDP over the diagonal PSD cone (scalar constraints become one-by-one LMIs)  
    - diag(Ax-b)â‰¼â‚›â‚Š0 enforces each component (Ax-b)áµ¢ â‰¤ 0 via 1x1 PSD blocks  
    - SOCP constraints â€–uâ€–â‚‚ â‰¤ t are equivalent to 2x2 PSD constraints on [ tI u; uáµ€ t ]  
    - Embedding LP and SOCP in SDP unifies all as conic problems over the PSD cone  
    - Enables use of general-purpose SDP solvers for a wider class of problems  
'''

##################################################################################

'''
================================================================================
Vector optimization
================================================================================

General vector optimization problem:
    minimize (w.r.t. K)    fâ‚€(x)         (w.r.t means "with respect to")
    subject to             fáµ¢(x) â‰¼ 0,    i = 1,â€¦,m  
                            háµ¢(x) = 0, i = 1,â€¦,p  

    â€¢ fâ‚€: â„â¿ â†’ â„áµ  is vector-valued  
    â€¢ â€œminimize w.r.t. Kâ€ means find x such that fâ‚€(x) is minimal under the partial order defined by proper cone K âŠ† â„áµ   
    â€¢ fáµ¢: â„â¿ â†’ â„ are scalar convex functions  

Convex vector optimization problem:
    minimize (w.r.t. K)    fâ‚€(x)
    subject to             fáµ¢(x) â‰¤ 0,   i = 1,â€¦,m  
                            A x = b  

    â€¢ fâ‚€ is K-convex: for all x,y and Î¸âˆˆ[0,1],  
        fâ‚€(Î¸x+(1-Î¸)y) â‰¼ Î¸fâ‚€(x)+(1-Î¸)fâ‚€(y)  
    â€¢ scalar constraints fáµ¢ convex, equality constraints affine  

Brief explanations:
    - Objectives are vector-valued; optimality means no other feasible point yields a strictly smaller vector in cone ordering  
    - Trade-offs between objectives characterized by Pareto frontier (set of nondominated solutions)  
    - Proper cone K (e.g., nonnegative orthant) defines preference direction in objective space  
    - Can be scalarized via weighted sums or Îµ-constraint methods to compute Pareto-optimal points  
    - Convexity ensures convex Pareto frontier and tractable computation of supported efficient points  
    - Applications: multi-criteria decision making, game theory, economics, engineering design

=================================================================================
Optimal and Pareto optimal points
=================================================================================

Set of achievable objective values:
    ð’ª = {fâ‚€(x) | x feasible}

Definitions:
    â€¢ Feasible x is optimal if fâ‚€(x) is the minimum value of ð’ª
    â€¢ Feasible x is Pareto optimal if fâ‚€(x) is a minimal value of ð’ª

Brief explanations:
    - Optimal point: global minimum with respect to cone ordering (unique if it exists)
    - Pareto optimal point: cannot be improved in all objectives simultaneously
    - Minimal value: no other achievable point y âˆˆ ð’ª satisfies y â‰º_K fâ‚€(x) (strictly dominated)
    - Minimum value: fâ‚€(x) â‰¼_K y for all y âˆˆ ð’ª (globally best)
    - Left diagram: single optimal point x* at lower-left corner of achievable set ð’ª
    - Right diagram: Pareto frontier forms boundary where no point dominates others
    - Pareto optimal points represent different trade-offs between conflicting objectives
    - In practice, decision makers choose among Pareto optimal solutions based on preferences
    - Convex problems have convex Pareto frontiers, making them easier to characterize

=================================================================================
Multicriterion (multi-objective) optimization
=================================================================================

Problem (vector optimization with K = â„â‚Šáµ ):
    fâ‚€(x) = (Fâ‚(x), â€¦, Fáµ©(x))  
    minimize (w.r.t. K)    fâ‚€(x)  
    subject to             x feasible (e.g., fáµ¢(x) â‰¤ 0, Ax = b)  

Definitions:
    â€¢ q different objectives Fáµ¢(x); we want all Fáµ¢ small  
    â€¢ Feasible x* is **optimal** if for all feasible y,  
        fâ‚€(x*) â‰¼ fâ‚€(y)  
      (objectives noncompeting; single global minimum)  
    â€¢ Feasible xáµ–áµ’ is **Pareto optimal** if for any feasible y,  
        fâ‚€(y) â‰¼ fâ‚€(xáµ–áµ’) â‡’ fâ‚€(y) = fâ‚€(xáµ–áµ’)  
      (no other y strictly improves all objectives)

Brief explanations:
    - **Optimal point** exists only if objectives align (non-conflicting)  
    - **Pareto optimal points** form the Pareto frontier where trade-offs occur  
    - No single solution minimizes all objectives if they conflict  
    - Solutions on Pareto frontier are **nondominated** under â„â‚Šáµ© ordering  
    - **Scalarization** (weighted sum, Îµ-constraint) converts to single-objective problems to compute supported Pareto points  
    - Convex multi-objective problems yield a convex Pareto frontier, facilitating efficient computation  
    - Applications: engineering design, economics, portfolio selection, policy planning  

================================================================================
Scalarization for Multi-Objective Optimization
================================================================================

Problem (weighted sum method):
    choose weight vector Î» â‰»â‚– 0  
    minimize    Î»áµ€fâ‚€(x)  
    subject to  fáµ¢(x) â‰¤ 0,    i = 1,â€¦,m  
                háµ¢(x) = 0,    i = 1,â€¦,p  

Brief explanations:
    - Î» âˆˆ â„â‚Šáµ  (for K = â„â‚Šáµ ) assigns relative importance to each objective  
    - The scalar objective Î»áµ€fâ‚€(x) is a convex combination when âˆ‘Î»â±¼ = 1  
    - Solving the scalar problem yields a Pareto-optimal solution of the original vector problem  
    - Different Î» directions correspond to supporting hyperplanes touching the Pareto frontier  
    - Varying Î» over the positive cone can recover (almost) all supported Pareto points in convex problems  
    - Unsupported (non-convex) portions of the Pareto frontier require other methods (Îµ-constraint, Bensonâ€™s algorithm)  
    - Weighted sum is simple to implement and leverages standard single-objective solvers  


================================================================================
Scalarization for Multicriterion Problems (Example)
================================================================================

Problem (weighted sum of two objectives):
    choose Î» = (1, Î³) with Î³ > 0  
    minimize    â€–Ax - bâ€–â‚‚Â² + Î³ â€–xâ€–â‚‚Â²  

Brief explanations:
    - This is a weighted sum of data-fitting (least-squares) and regularization objectives  
    - Î³ controls the trade-off: larger Î³ places more weight on keeping x small (regularization)  
    - For each fixed Î³, the problem is a standard least-squares problem with Tikhonov (â„“â‚‚) regularization  
    - Solutions for different Î³ trace out the Pareto frontier between fitting error and solution norm  
    - The curve in objective space shows achievable pairs (â€–Ax-bâ€–â‚‚Â², â€–xâ€–â‚‚Â²) and their trade-offs  
    - At Î³=1, the tangent to the Pareto frontier corresponds to weight vector Î»=(1,1)  
    - Scalarization leverages efficient linear-algebra solvers to compute Pareto-optimal points  

'''